{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Delta Time Travel Demo\n",
    "* Demonstrate OSS Delta time travel features\n",
    "* This notebook works both as a Jupyter or Databricks notebook\n",
    "* Jupyter\n",
    "  * For Jupyter we assume you have Spark installed on your laptop\n",
    "  * Works with OSS Delta\n",
    "  * Note: Delta SQL does not work in OSS\n",
    "* Databricks\n",
    "  * For Databricks path names are `dbfs:`\n",
    "  * Works with full Databricks Delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prelude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"TimeTravel\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ.get('PYSPARK_SUBMIT_ARGS',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_databricks = 'DATABRICKS_RUNTIME_VERSION' in os.environ\n",
    "in_databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"cats\"\n",
    "if in_databricks:\n",
    "  user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get(\"user\").get()\n",
    "  user = user.split(\"@\")[0].replace(\".\",\"_\")\n",
    "  database = f\"{user}_delta_fun\"\n",
    "  dataPath = f\"dbfs:/tmp/{user}/delta_fun/table_cats\"\n",
    "else:\n",
    "  database = \"delta_fun\"\n",
    "  dataPath = \"delta_fun\"\n",
    "database,dataPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def now(): \n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(time.time()))\n",
    "now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert(path, data, mode=\"append\"):\n",
    "    df = spark.createDataFrame(data, [\"id\",\"name\",\"region\"])\n",
    "    df.coalesce(1).write.mode(mode).format(\"delta\").save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert three batches of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert(dataPath, [\n",
    "    (1, \"lion\",\"africa\"),\n",
    "    (2, \"cheetah\",\"africa\"),\n",
    "    (3, \"leopard\",\"africa\")], \"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert(dataPath, [\n",
    "    (4, \"jaguar\",\"south america\"),\n",
    "    (5, \"puma\",\"south america\"),\n",
    "    (6, \"ocelot\",\"south america\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert(dataPath, [\n",
    "    (7, \"lynx\",\"north america\"),\n",
    "    (8, \"bobcat\",\"north america\"),\n",
    "    (9, \"catamount\",\"north america\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if in_databricks:\n",
    "  for f in dbutils.fs.ls(dataPath):\n",
    "    print(f.size,f.name)\n",
    "else:\n",
    "  for f in os.listdir(dataPath):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(dataPath)\n",
    "df.sort(\"id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display versioned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in range(0,3):\n",
    "    print(\"Version\",v)\n",
    "    df = spark.read.format(\"delta\").option(\"versionAsOf\", str(v)).load(dataPath)\n",
    "    df.sort(\"id\").show(100,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Delta with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"create database if not exists {database}\")\n",
    "spark.sql(f\"use {database}\")\n",
    "spark.sql(f\"drop table if exists {table}\")\n",
    "df = spark.sql(\"show databases\").filter(f\"databaseName = '{database}'\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"create table {table} using delta location '{dataPath}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"describe formatted {table}\").show(1000,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select * from {table} order by id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Databricks-specific queries\n",
    "These do not work on OSS Delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if in_databricks: spark.sql(f\"describe history {table}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if in_databricks: spark.sql(f\"describe detail {table}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if in_databricks: \n",
    "    for v in range(0,3):\n",
    "        print(\"Version\",v)\n",
    "        spark.sql(f\"select * from {table} version as of {v} order by id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "name": "time_travel_Python",
  "notebookId": 3435730
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
